{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('data/askreddit_dt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>gilded</th>\n",
       "      <th>author</th>\n",
       "      <th>created</th>\n",
       "      <th>is_submitter</th>\n",
       "      <th>permalink</th>\n",
       "      <th>score</th>\n",
       "      <th>body</th>\n",
       "      <th>topic</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f3snxvl</td>\n",
       "      <td>t3_di0g3m</td>\n",
       "      <td>t3_di0g3m</td>\n",
       "      <td>None</td>\n",
       "      <td>UltraMiner245</td>\n",
       "      <td>2019-10-14 21:18:17</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/AskReddit/comments/di0g3m/if_you_died_the_e...</td>\n",
       "      <td>869</td>\n",
       "      <td>Being blown up by the Vex</td>\n",
       "      <td>If you died the exact same way as the last tim...</td>\n",
       "      <td>2019-10-14 21:18:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f3sgewt</td>\n",
       "      <td>t3_di0g3m</td>\n",
       "      <td>t3_di0g3m</td>\n",
       "      <td>None</td>\n",
       "      <td>talibob</td>\n",
       "      <td>2019-10-14 20:20:25</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/AskReddit/comments/di0g3m/if_you_died_the_e...</td>\n",
       "      <td>1116</td>\n",
       "      <td>Getting punched in the face by an angry demon ...</td>\n",
       "      <td>If you died the exact same way as the last tim...</td>\n",
       "      <td>2019-10-14 20:20:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f3sh6qo</td>\n",
       "      <td>t3_di0g3m</td>\n",
       "      <td>t3_di0g3m</td>\n",
       "      <td>None</td>\n",
       "      <td>toastyhoodie</td>\n",
       "      <td>2019-10-14 20:26:19</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/AskReddit/comments/di0g3m/if_you_died_the_e...</td>\n",
       "      <td>2075</td>\n",
       "      <td>That damned lava in Mario Maker</td>\n",
       "      <td>If you died the exact same way as the last tim...</td>\n",
       "      <td>2019-10-14 20:26:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f3sjrei</td>\n",
       "      <td>t3_di0g3m</td>\n",
       "      <td>t3_di0g3m</td>\n",
       "      <td>None</td>\n",
       "      <td>eDwArDdOoMiNgToN</td>\n",
       "      <td>2019-10-14 20:46:19</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/AskReddit/comments/di0g3m/if_you_died_the_e...</td>\n",
       "      <td>691</td>\n",
       "      <td>Falling from space and burning up in a go kart...</td>\n",
       "      <td>If you died the exact same way as the last tim...</td>\n",
       "      <td>2019-10-14 20:46:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f3solw6</td>\n",
       "      <td>t3_di0g3m</td>\n",
       "      <td>t3_di0g3m</td>\n",
       "      <td>None</td>\n",
       "      <td>bumblehoneyb</td>\n",
       "      <td>2019-10-14 21:23:21</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/AskReddit/comments/di0g3m/if_you_died_the_e...</td>\n",
       "      <td>927</td>\n",
       "      <td>exhaustion from over-working myself in blind a...</td>\n",
       "      <td>If you died the exact same way as the last tim...</td>\n",
       "      <td>2019-10-14 21:23:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  parent_id    link_id gilded            author  \\\n",
       "0  f3snxvl  t3_di0g3m  t3_di0g3m   None     UltraMiner245   \n",
       "1  f3sgewt  t3_di0g3m  t3_di0g3m   None           talibob   \n",
       "2  f3sh6qo  t3_di0g3m  t3_di0g3m   None      toastyhoodie   \n",
       "3  f3sjrei  t3_di0g3m  t3_di0g3m   None  eDwArDdOoMiNgToN   \n",
       "4  f3solw6  t3_di0g3m  t3_di0g3m   None      bumblehoneyb   \n",
       "\n",
       "               created is_submitter  \\\n",
       "0  2019-10-14 21:18:17         None   \n",
       "1  2019-10-14 20:20:25         None   \n",
       "2  2019-10-14 20:26:19         None   \n",
       "3  2019-10-14 20:46:19         None   \n",
       "4  2019-10-14 21:23:21         None   \n",
       "\n",
       "                                           permalink score  \\\n",
       "0  /r/AskReddit/comments/di0g3m/if_you_died_the_e...   869   \n",
       "1  /r/AskReddit/comments/di0g3m/if_you_died_the_e...  1116   \n",
       "2  /r/AskReddit/comments/di0g3m/if_you_died_the_e...  2075   \n",
       "3  /r/AskReddit/comments/di0g3m/if_you_died_the_e...   691   \n",
       "4  /r/AskReddit/comments/di0g3m/if_you_died_the_e...   927   \n",
       "\n",
       "                                                body  \\\n",
       "0                          Being blown up by the Vex   \n",
       "1  Getting punched in the face by an angry demon ...   \n",
       "2                    That damned lava in Mario Maker   \n",
       "3  Falling from space and burning up in a go kart...   \n",
       "4  exhaustion from over-working myself in blind a...   \n",
       "\n",
       "                                               topic            timestamp  \n",
       "0  If you died the exact same way as the last tim...  2019-10-14 21:18:17  \n",
       "1  If you died the exact same way as the last tim...  2019-10-14 20:20:25  \n",
       "2  If you died the exact same way as the last tim...  2019-10-14 20:26:19  \n",
       "3  If you died the exact same way as the last tim...  2019-10-14 20:46:19  \n",
       "4  If you died the exact same way as the last tim...  2019-10-14 21:23:21  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/askreddit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>gilded</th>\n",
       "      <th>author</th>\n",
       "      <th>created</th>\n",
       "      <th>is_submitter</th>\n",
       "      <th>permalink</th>\n",
       "      <th>score</th>\n",
       "      <th>body</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f3snxvl</td>\n",
       "      <td>t3_di0g3m</td>\n",
       "      <td>t3_di0g3m</td>\n",
       "      <td>None</td>\n",
       "      <td>UltraMiner245</td>\n",
       "      <td>1.571106e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/AskReddit/comments/di0g3m/if_you_died_the_e...</td>\n",
       "      <td>869</td>\n",
       "      <td>Being blown up by the Vex</td>\n",
       "      <td>If you died the exact same way as the last tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f3sgewt</td>\n",
       "      <td>t3_di0g3m</td>\n",
       "      <td>t3_di0g3m</td>\n",
       "      <td>None</td>\n",
       "      <td>talibob</td>\n",
       "      <td>1.571102e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/AskReddit/comments/di0g3m/if_you_died_the_e...</td>\n",
       "      <td>1116</td>\n",
       "      <td>Getting punched in the face by an angry demon ...</td>\n",
       "      <td>If you died the exact same way as the last tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f3sh6qo</td>\n",
       "      <td>t3_di0g3m</td>\n",
       "      <td>t3_di0g3m</td>\n",
       "      <td>None</td>\n",
       "      <td>toastyhoodie</td>\n",
       "      <td>1.571103e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/AskReddit/comments/di0g3m/if_you_died_the_e...</td>\n",
       "      <td>2075</td>\n",
       "      <td>That damned lava in Mario Maker</td>\n",
       "      <td>If you died the exact same way as the last tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f3sjrei</td>\n",
       "      <td>t3_di0g3m</td>\n",
       "      <td>t3_di0g3m</td>\n",
       "      <td>None</td>\n",
       "      <td>eDwArDdOoMiNgToN</td>\n",
       "      <td>1.571104e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/AskReddit/comments/di0g3m/if_you_died_the_e...</td>\n",
       "      <td>691</td>\n",
       "      <td>Falling from space and burning up in a go kart...</td>\n",
       "      <td>If you died the exact same way as the last tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f3solw6</td>\n",
       "      <td>t3_di0g3m</td>\n",
       "      <td>t3_di0g3m</td>\n",
       "      <td>None</td>\n",
       "      <td>bumblehoneyb</td>\n",
       "      <td>1.571106e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/AskReddit/comments/di0g3m/if_you_died_the_e...</td>\n",
       "      <td>927</td>\n",
       "      <td>exhaustion from over-working myself in blind a...</td>\n",
       "      <td>If you died the exact same way as the last tim...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  parent_id    link_id gilded            author       created  \\\n",
       "0  f3snxvl  t3_di0g3m  t3_di0g3m   None     UltraMiner245  1.571106e+09   \n",
       "1  f3sgewt  t3_di0g3m  t3_di0g3m   None           talibob  1.571102e+09   \n",
       "2  f3sh6qo  t3_di0g3m  t3_di0g3m   None      toastyhoodie  1.571103e+09   \n",
       "3  f3sjrei  t3_di0g3m  t3_di0g3m   None  eDwArDdOoMiNgToN  1.571104e+09   \n",
       "4  f3solw6  t3_di0g3m  t3_di0g3m   None      bumblehoneyb  1.571106e+09   \n",
       "\n",
       "  is_submitter                                          permalink score  \\\n",
       "0         None  /r/AskReddit/comments/di0g3m/if_you_died_the_e...   869   \n",
       "1         None  /r/AskReddit/comments/di0g3m/if_you_died_the_e...  1116   \n",
       "2         None  /r/AskReddit/comments/di0g3m/if_you_died_the_e...  2075   \n",
       "3         None  /r/AskReddit/comments/di0g3m/if_you_died_the_e...   691   \n",
       "4         None  /r/AskReddit/comments/di0g3m/if_you_died_the_e...   927   \n",
       "\n",
       "                                                body  \\\n",
       "0                          Being blown up by the Vex   \n",
       "1  Getting punched in the face by an angry demon ...   \n",
       "2                    That damned lava in Mario Maker   \n",
       "3  Falling from space and burning up in a go kart...   \n",
       "4  exhaustion from over-working myself in blind a...   \n",
       "\n",
       "                                               topic  \n",
       "0  If you died the exact same way as the last tim...  \n",
       "1  If you died the exact same way as the last tim...  \n",
       "2  If you died the exact same way as the last tim...  \n",
       "3  If you died the exact same way as the last tim...  \n",
       "4  If you died the exact same way as the last tim...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12157"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df2['gilded'] == \"None\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12169"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['link_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization, Stop Word Removal, Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll = df2['body'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Being blown up by the Vex',\n",
       " \"Getting punched in the face by an angry demon ape. That's a terrifying thought.  Edit: Yes, it was Rajang from Monster Hunter.\",\n",
       " 'That damned lava in Mario Maker',\n",
       " 'Falling from space and burning up in a go kart.\\n\\nMario Kart Wii btw',\n",
       " 'exhaustion from over-working myself in blind ambition\\n\\n\\\\-Stardew Valley, if it counts']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = [word_tokenize(comment.lower()) for comment in coll]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['being', 'blown', 'up', 'by', 'the', 'vex'],\n",
       " ['getting',\n",
       "  'punched',\n",
       "  'in',\n",
       "  'the',\n",
       "  'face',\n",
       "  'by',\n",
       "  'an',\n",
       "  'angry',\n",
       "  'demon',\n",
       "  'ape',\n",
       "  '.',\n",
       "  'that',\n",
       "  \"'s\",\n",
       "  'a',\n",
       "  'terrifying',\n",
       "  'thought',\n",
       "  '.',\n",
       "  'edit',\n",
       "  ':',\n",
       "  'yes',\n",
       "  ',',\n",
       "  'it',\n",
       "  'was',\n",
       "  'rajang',\n",
       "  'from',\n",
       "  'monster',\n",
       "  'hunter',\n",
       "  '.'],\n",
       " ['that', 'damned', 'lava', 'in', 'mario', 'maker'],\n",
       " ['falling',\n",
       "  'from',\n",
       "  'space',\n",
       "  'and',\n",
       "  'burning',\n",
       "  'up',\n",
       "  'in',\n",
       "  'a',\n",
       "  'go',\n",
       "  'kart',\n",
       "  '.',\n",
       "  'mario',\n",
       "  'kart',\n",
       "  'wii',\n",
       "  'btw'],\n",
       " ['exhaustion',\n",
       "  'from',\n",
       "  'over-working',\n",
       "  'myself',\n",
       "  'in',\n",
       "  'blind',\n",
       "  'ambition',\n",
       "  '\\\\-stardew',\n",
       "  'valley',\n",
       "  ',',\n",
       "  'if',\n",
       "  'it',\n",
       "  'counts']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "docs = [[word for word in words if word not in stop] for words in tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['blown', 'vex'],\n",
       " ['getting',\n",
       "  'punched',\n",
       "  'face',\n",
       "  'angry',\n",
       "  'demon',\n",
       "  'ape',\n",
       "  '.',\n",
       "  \"'s\",\n",
       "  'terrifying',\n",
       "  'thought',\n",
       "  '.',\n",
       "  'edit',\n",
       "  ':',\n",
       "  'yes',\n",
       "  ',',\n",
       "  'rajang',\n",
       "  'monster',\n",
       "  'hunter',\n",
       "  '.'],\n",
       " ['damned', 'lava', 'mario', 'maker'],\n",
       " ['falling',\n",
       "  'space',\n",
       "  'burning',\n",
       "  'go',\n",
       "  'kart',\n",
       "  '.',\n",
       "  'mario',\n",
       "  'kart',\n",
       "  'wii',\n",
       "  'btw'],\n",
       " ['exhaustion',\n",
       "  'over-working',\n",
       "  'blind',\n",
       "  'ambition',\n",
       "  '\\\\-stardew',\n",
       "  'valley',\n",
       "  ',',\n",
       "  'counts']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "docs_stemd = [[porter.stem(word) for word in words] for words in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['blown', 'vex'],\n",
       " ['get',\n",
       "  'punch',\n",
       "  'face',\n",
       "  'angri',\n",
       "  'demon',\n",
       "  'ape',\n",
       "  '.',\n",
       "  \"'s\",\n",
       "  'terrifi',\n",
       "  'thought',\n",
       "  '.',\n",
       "  'edit',\n",
       "  ':',\n",
       "  'ye',\n",
       "  ',',\n",
       "  'rajang',\n",
       "  'monster',\n",
       "  'hunter',\n",
       "  '.'],\n",
       " ['damn', 'lava', 'mario', 'maker'],\n",
       " ['fall', 'space', 'burn', 'go', 'kart', '.', 'mario', 'kart', 'wii', 'btw'],\n",
       " ['exhaust',\n",
       "  'over-work',\n",
       "  'blind',\n",
       "  'ambit',\n",
       "  '\\\\-stardew',\n",
       "  'valley',\n",
       "  ',',\n",
       "  'count']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_stemd[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words and TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_set = set()\n",
    "[[vocab_set.add(token) for token in tokens] for tokens in docs_stemd]\n",
    "vocab = list(vocab_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['battlefleet',\n",
       " 'fresh',\n",
       " '.♪',\n",
       " 'testifi',\n",
       " 'cold-snap',\n",
       " 'horn',\n",
       " '3.',\n",
       " 'bicycl',\n",
       " 'voort',\n",
       " 'metter']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a reverse lookup for the vocab list. \n",
    "# This is a dictionary whose keys are the words and values are the indices of the words (the word id). \n",
    "# This will make things much faster than using the list index function\n",
    "vocab_dict = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Each row is a doc, each column a word, each value a count of how many times that word appeared in that doc\n",
    "word_counts = np.zeros((len(docs), len(vocab)))\n",
    "for doc_id, words in enumerate(docs_stemd):\n",
    "    for word in words:\n",
    "        word_id = vocab_dict[word]\n",
    "        word_counts[doc_id][word_id] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document frequencies: for each word, get a count of the number of documents the word appears in\n",
    "doc_fr = np.sum(word_counts > 0, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the word count matrix to get the term frequencies. \n",
    "# This means dividing each count by the L1 norm (the sum of all the counts). \n",
    "# This makes each vector a vector of term frequencies.\n",
    "tf_norm = word_counts.sum(axis=1)\n",
    "tf_norm[tf_norm == 0] = 1\n",
    "tf = word_counts / tf_norm.reshape(len(docs_stemd), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF matrix: Multiply the term frequency matrix by the log of the inverse of the document frequences\n",
    "idf = np.log((len(docs_stemd) + 1.) / (1. + doc_fr)) + 1.\n",
    "tfidf = tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the tf-idf matrix as well by dividing by the L2 norm.\n",
    "tfidf_norm = np.sqrt((tfidf ** 2).sum(axis=1))\n",
    "tfidf_norm[tfidf_norm == 0] = 1\n",
    "tfidf_normed = tfidf / tfidf_norm.reshape(len(docs_stemd), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc):\n",
    "    return [porter.stem(word) for word in word_tokenize(doc.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "countvect = CountVectorizer(stop_words='english', tokenizer=tokenize)\n",
    "count_vectorized = countvect.fit_transform(coll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "tfidfvect = TfidfVectorizer(stop_words='english', tokenizer=tokenize)\n",
    "tfidf_vectorized = tfidfvect.fit_transform(coll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
